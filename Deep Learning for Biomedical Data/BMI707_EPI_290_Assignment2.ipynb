{"cells":[{"cell_type":"markdown","metadata":{"id":"hjgxJAFHU_LU"},"source":["## Assignment 2 - Convolutional Neural Networks (CNNs)"]},{"cell_type":"markdown","metadata":{"id":"y30mEZgtVBtB"},"source":["Your Name: Nezih Nieto GutiÃ©rrez\n","\n","Name of the Students You Worked With:\n","\n","References You Consulted:\n","\n","\n","https://github.com/nachi-hebbar/Transfer-Learning-Keras/blob/main/TransferLearning.ipynb\n","\n","\n","https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html\n","\n","\n","https://keras.io/guides/sequential_model/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IX7rv7cZcqtp"},"outputs":[],"source":["from PIL import Image\n","import numpy as np\n","import pandas as pd\n","import os\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YDST8U9VWoz2"},"source":["# Part 1: Convolutional Neural Networks for Chest X-Ray Images (80 points)\n","## Data and Preliminaries\n","In this assignment, we will explore the NIH CXR8 database (https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community). This chest X-ray database is the basis of a series of highly-cited papers/preprint papers. You will examine the images in this dataset and build your own convolutional neural networks to classify the major diagnoses."]},{"cell_type":"markdown","metadata":{"id":"JCZRIiEfVG0N"},"source":["## Question 1: Data summary (10 points)\n","Deep learning is not immune to the 'garbage in, garbage out' principle. Before digging into the data, it is recommended to get a sense of how the data was generated, understand the assumptions of the data, and review the data quality. We will ask you to answer some basic questions on the NIH CXR8 dataset. Please visit the website of the NIH CXR8 database, download the metadata (Data_Entry_2017.csv; https://nihcc.app.box.com/v/ChestXray-NIHCC/file/219760887468), and answer the following questions."]},{"cell_type":"markdown","metadata":{"id":"gXtVh3IrVJgB"},"source":["####Question 1.1 (5 points)\n","What is the file format of images in the NIH CXR8 database? What is the standard format for radiology image storage and transmission? How many images are there in the database? How many diagnostic categories are there in the database? What are they? How many images have more than one diagnosis?\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPgEsuu1ZNj-"},"outputs":[],"source":["\n","#!wget https://nihcc.app.box.com/v/ChestXray-NIHCC/file/219760887468\n","\n","!wget -O Data_Entry_2017_v2020.csv \thttps://public.boxcloud.com/d/1/b1!NBOZDmyhYVPBaFDiUhZV9HH0aLDoazjcLI03yGHLYYrua9TiH4vTVvluk-DDMcA9mt2p8w4eBgPpQ4p37P687TNZZ2u3nQWZda822NGjH0jebzY3--mifSJuseSMDIodgKFiJP4f7tZL9YdSWMJtGKYLHz2nj4BlTrzSSuw5m0NwLRXANPfCQjIhEb-SHzXWSWURjm62bm0gdIRfy3zzVkNZ1XbqzdjH4TmlLMJDdubBk7YnLvtssBtYzH9vbDJu1-sCAgqZDubeTeAo262QWPibzNmaer6zoS6OZRSix95Yi10-8DX-Xv5s8Gi7n16_FFsNUVIEJ0mtp8yu3lztyTluGRLLeKqPRHcaLYhxAyGqCLsuf9p7fPc1KqrQrFllKR3s2dNy7P0HjwIOb0TVUqNJEuCEXnt0oOQ8zlDz2CtbJ2gfZZ_4wthAG4SS6cnh33aqFT4cPjgAnirqvAPQiKf3HJKZQpxj4Vi-3-HBOsGY_fqa5On9tuOeVe-OjvIA_lFZ6ctqi8uGr6wyJvaY_jnxhBSvb_5DGGabP4pEFcX-iVC-raNZGJxLR7rJ4Kncc6oSrer05HbElvZyHFdH1CNvMJL65PkvxWSHbf3zMqbxJe40c7ALvE_cuwRMWMShUYHDcqYeZuSiBjoQjrbEZLQVzo4XGSDoGO16F639AxISVsOhVFTmsu1qhSYPSkUiqQgMm49zMhe9QYLhsPbkXLJZkXUWCEof4uYiyL2qRNNw-6caVR1vrMJ-V0KBo_E4PMMocMsLIDrBQShsVPO9V97s0TLzXuIzBPe6DiotLyZZA2xOFNHPxdxGVgiWigP18Bwso3-Lg18R9a1KOCP7P5I8uTbZbkfIHIAeaJR7_9EJTELP4EKOLvUVg59AXEH5remUBRAAUIsENnLiDcmnG94VqIoNwh4WLEaKncGhFMnF-5hzH1sd9R58-2TOBYAWsz9xK2Tsjp1TDFaFRB6JFgDSulr5Pogy6-t9EDF_5SNQSqQP0cQoGo-SCIAVyAOJ9DHmoVEUOnqiKs38uDZQeZI_3Kn-UhDLm0920OiKnZcwBl0D6ThRx4C2ltXiUKrFiWnufr-sCvD8T4Nh7pQKMUb4zoIaN9NYoRP4or5C0rvE3qrBsPpA_NlvffKHyeF6h3uf6kHjc1Ou1JI7VzusiMXucV3kGXh3TeAm1Wcqudr98pqtwi1YIMFBGT9934HjeTa1v8Ws2IxyjqztygBRnjT7-LXTMGXrO4YtmSfhsgpfg5wGuk0VsqATimzN/download"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_qDsDvId7NQ"},"outputs":[],"source":["Data_Entry = pd.read_csv('Data_Entry_2017_v2020.csv', low_memory=False)\n","Data_Entry"]},{"cell_type":"markdown","metadata":{"id":"rPUdn7NOVMJy"},"source":["__Your Answer:__\n","\n","\n","As seen in the data set, the images use a PNG format, nonetheless the most famous format in medicine is the DICOM (Digital Imaging and Communications in Medicine). This dataset contains 112120 images."]},{"cell_type":"markdown","metadata":{"id":"q0HLsn3uVOR1"},"source":["#### Question 1.2 (5 points)\n","How many patients are there in total? How many patients have more than one image? Which patient ID contributed the most images and how many did they contribute?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_vN0OuAngIVP"},"outputs":[],"source":["\n","Patients_Count=Data_Entry.groupby('Patient ID').count()\n","max_num_images=Patients_Count['Image Index'].max()\n","max_index=Patients_Count.index[Patients_Count['Image Index']==max_num_images].tolist()[0]\n","num_just1_image=Patients_Count['Image Index'].value_counts()[1]\n","num_moreThan1_image=Patients_Count.count()['Image Index']-num_just1_image\n","\n","\n","print('Max Num. of Images: %d'% max_num_images)\n","print('Max Num. of Images Patient ID: %d'% max_index)\n","print('Num of Patients with more than 1 image: %d'% num_moreThan1_image)\n","print('Total Patients: %d'%Patients_Count.count()['Image Index'])"]},{"cell_type":"markdown","metadata":{"id":"19ppgFt0VRUY"},"source":["__Your Answer:__\n","\n","\n","There is a total of 30805 patients, from which only 13302 patients (~43.18%) have more than 1 image. The one with the ID 10007 has the highest number of images, which is 184."]},{"cell_type":"markdown","metadata":{"id":"JyVq0OzcVTfH"},"source":["## Question 2: Check the images (10 points)\n","In the following questions, you will be asked to examine the images in the NIH CXR8 dataset. The image for Questions 2.1 could be found at https://www.dropbox.com/sh/2h068ge9xv1g27u/AAAXVq8VYXF6HRlHvzvjy-e6a?dl=0. \n","\n","Feel free to collaborate with other students or consult any references. For example, this blog post (https://lukeoakdenrayner.wordpress.com/2018/01/24/chexnet-an-in-depth-review/) and some related discussions (https://arxiv.org/abs/1907.12720) may provide some contexts."]},{"cell_type":"markdown","metadata":{"id":"AZgIj69yVVhC"},"source":["#### Question 2.1 (5 points)\n","What is the NIH-labeled diagnosis of image `00001583_014.png`? What is the medical device visible on the right side of the patient (the left side of the image)? What are some concerns about machine learning models trained by images like this one? Word limit: 200 words."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sw_dL6KMrYSK"},"outputs":[],"source":["\n","!wget -O Q2.zip\thttps://www.dropbox.com/sh/2h068ge9xv1g27u/AAAXVq8VYXF6HRlHvzvjy-e6a?dl=0\n","!unzip Q2.zip\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pH1leLvPnZlX"},"outputs":[],"source":["index=Data_Entry.index[Data_Entry['Image Index']=='00001583_014.png'].tolist()\n","\n","diagnosis=Data_Entry['Finding Labels'].loc[index].tolist()[0]\n","patient=Data_Entry['Patient ID'].loc[index].tolist()[0]\n","\n","print('The diagnosis from Image 00001583_014.png from Patient %d'%patient,'is '+diagnosis+'\\n')\n","\n","import cv2 as cv\n","from matplotlib import pyplot as plt\n","\n","\n","img = cv.imread('00001583_014.png')\n","plt.imshow(img, cmap='gray')\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nz3w83OSkZDi"},"source":["__Your Answer:__"]},{"cell_type":"markdown","metadata":{"id":"waQr4FscVZOQ"},"source":["#### Question 2.2 (5 points)\n","What is \"View Position?\" How does it affect the resulting chest X-ray images visually? Word limit: 100 words."]},{"cell_type":"markdown","metadata":{"id":"EK3dNZigkcE7"},"source":["__Your Answer:__\n","\n","\n","View position refers to the posture of the subjects that undergo a chest X-ray study. It would affect considerably the dimensions or the features of the image at different regions within the torax.  The most popular View Positions are Anterioposterior (AP) and Posterioranterior (PA), which means from a rear view or from the back and from a frontal view respectively.  "]},{"cell_type":"markdown","metadata":{"id":"pSSXlbA7VbLo"},"source":["## Question 3: Build a custom convolutional neural network (15 points)\n","For this question, we ask you to build a multi-layer convolutional neural network to classify a subset of cardiomegaly images from normal ones. Please download the training set and the validation set here (https://www.dropbox.com/sh/ojiw79q8786ua4x/AAAtaJVKEdv91Zybpi-fAfMsa?dl=0). Please DO NOT use any other image from NIH CXR8 or other databases for this question. Feel free to use keras or any other high-level deep learning packages to classify the images.\n","\n","Design a convolutional neural network with at least two convolution layers, at least one max-pooling layer, and at least one dropout layer. Although you should explore various combinations of hyperparameters, we will grade this question based on the accuracy of the implementation, not the performance of the network.\n","\n","What is your design? What binary loss/accuracy did you get in the training and validation set? Please include your code in the assignment submission.\n","\n","Your neural network should have an AUC >= 0.55 when evaluated by the validation data set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3o8adpcs6XG"},"outputs":[],"source":["!wget -O Q3Q4.zip https://www.dropbox.com/sh/ojiw79q8786ua4x/AAAtaJVKEdv91Zybpi-fAfMsa?dl=0\n","!unzip Q3Q4.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QFVCj-wwtVSF"},"outputs":[],"source":["# decompress the tar files\n","!tar -xf converted_to_RGB/train.tar\n","!tar -xf converted_to_RGB/val.tar\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6tAWhnvU7sm"},"outputs":[],"source":["\n","#@title\n","## Your code goes here\n","################################################################################\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","import os\n","\n","# read in the labels\n","train_labels = pd.read_csv('train.csv',  header=None, index_col=0)\n","val_labels = pd.read_csv('val.csv',  header=None, index_col=0)\n","\n","# we will resize the images to make them smaller\n","# feel free to adjust this step in your assignment submission\n","image_size = (128,128)\n","\n","\n","# read in the training images\n","train_images = []\n","train_dir = './train/'\n","train_files = os.listdir(train_dir)\n","for f in train_files:\n","  img = Image.open(train_dir + f)\n","  img = img.resize(image_size)\n","  img_arr = np.array(img)\n","  train_images.append(img_arr)\n","\n","train_X = np.array(train_images)\n","\n","# read in the val images\n","val_images = []\n","val_dir = './val/'\n","val_files = os.listdir(val_dir)\n","for f in val_files:\n","  img = Image.open(val_dir + f)\n","  img = img.resize(image_size)\n","  img_arr = np.array(img)\n","  val_images.append(img_arr)\n","\n","val_X = np.array(val_images)\n","\n","\n","# reorder the labels so that they line up with the order of the image files\n","train_labels = train_labels.reindex(train_files)\n","val_labels = val_labels.reindex(val_files)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DH7WaS5ht-Kr"},"outputs":[],"source":["from sklearn.preprocessing import LabelBinarizer\n","label_transformer = LabelBinarizer()\n","train_y = label_transformer.fit_transform(train_labels)\n","val_y = label_transformer.transform(val_labels)\n","\n","# adding a dimension for channel\n","train_X = np.expand_dims(train_X, 3)\n","val_X = np.expand_dims(val_X, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9E2phjmuIcg"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.optimizers import Nadam\n","model = Sequential()\n","model.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=(train_X.shape[1:])))\n","model.add(Dropout(0.2))\n","model.add(Conv2D(filters=40, kernel_size=(5,5)))\n","model.add(Dropout(0.2))\n","model.add(Conv2D(filters=32, kernel_size=(4,4)))\n","model.add(Dropout(0.2))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","\n","opt = Adam(learning_rate=0.001)\n","\n","model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy','AUC'])\n","\n","model.fit(train_X, train_y, validation_data=(val_X,val_y), epochs=30, batch_size=32)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbxEDjjtuzBP"},"outputs":[],"source":["model.evaluate(val_X,val_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VblNoo5x6di0"},"outputs":[],"source":["\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"xkXSpQihVjUB"},"source":["__Your Answer:__\n","\n","\n","My design contains a multilayer CNN shown above, with 3 2D COnvolutional layers, 3 Dropout layers, one 2D Maxpooling layer, and a flattening filter. Finally, it ends with a Dense layer to correct the dimensions of the output which works with a sigmoid activation function.\n","\n","\n","Resulting values for the validation dataset are aproximately:\n","\n","\n","loss: 6.6434 - accuracy: 0.6316 - auc: 0.6504\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0wsYHswwVlYn"},"source":["## Question 4: Transfer learning: Using the VGGNet (16 layers) architecture (20 points)\n","For this question, we ask you to employ VGGNet, a convolutional neural network built for ImageNet, to classify the same subset of cardiomegaly images from normal ones (https://www.dropbox.com/sh/ojiw79q8786ua4x/AAAtaJVKEdv91Zybpi-fAfMsa?dl=0). We encourage you to take a look at the documentation for keras.applications (https://keras.io/applications/) and reuse their modules. Please DO NOT use any other images from NIH CXR8 or from other databases for this question. Although you should explore various combinations of hyperparameters, we will grade this question based on the accuracy of the implementation, not the performance of the network.\n","\n","Your neural network should have an AUC >= 0.7 when evaluated by the validation data set."]},{"cell_type":"markdown","metadata":{"id":"kICXlS7tVnLJ"},"source":["### Question 4.1 (10 points)\n","What is your best validation accuracy of fine-tuning a 16-layer VGGNet WITHOUT ImageNet weights? In your model with the lowest validation loss, what are the hyperparameters? What is the validation loss? What is the training loss/accuracy? Please include your code in the assignment submission."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SfxJ3JFEkiv4"},"outputs":[],"source":["#@title\n","## Your code goes here\n","################################################################################\n","import tensorflow as tf\n","import keras \n","import os\n","import cv2 as cv\n","from matplotlib import pyplot as plt\n","\n","\n","\n","from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","import numpy as np\n","from glob import glob\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","     \n","\n","IMAGE_SIZE=[128,128]\n","image_size=(128,128)\n","BATCH_SIZE = 32\n"]},{"cell_type":"code","source":["cardiomagalyList=Data_Entry.index[Data_Entry['Finding Labels']=='Cardiomegaly'].tolist()\n","cardio=Data_Entry.iloc[cardiomagalyList]['Image Index'].tolist()\n","class_subset=[]\n","class_subset_prime = []\n","\n","\n","train_dir = './train/'\n","val_dir = './val/'\n","train_files = os.listdir(train_dir)\n","val_files = os.listdir(val_dir)\n","\n","! mkdir Cardiomegaly\n","! mkdir ./Cardiomegaly/train\n","! mkdir ./Cardiomegaly/train/class1\n","! mkdir ./Cardiomegaly/train/class2\n","! mkdir ./Cardiomegaly/val\n","! mkdir ./Cardiomegaly/val/class1\n","! mkdir ./Cardiomegaly/val/class2\n"],"metadata":{"id":"STaLi41NJaDy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XbBF8Wy9jA5u"},"outputs":[],"source":["for f in train_files:\n","  for g in cardio:\n","    if(f==g):\n","      class_subset.append(g)\n","      img = Image.open(train_dir + g)\n","      img.save(\"./Cardiomegaly/train/class1/\"+g)\n","      img.close()\n","    else:\n","      img = Image.open(train_dir + f)\n","      img.save(\"./Cardiomegaly/train/class2/\"+f)\n","      img.close()\n","\n"]},{"cell_type":"code","source":["\n","for f in val_files:\n","  for g in cardio[:20]:\n","    if(f==g):\n","      class_subset_prime.append(g)\n","      img = Image.open(val_dir + g)\n","      img.save(\"./Cardiomegaly/val/class1/\"+g)\n","      img.close()\n","    else:\n","      img = Image.open(val_dir + f)\n","      img.save(\"./Cardiomegaly/val/class2/\"+f)\n","      img.close()\n"],"metadata":{"id":"cnuQzioLRTSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZTJPqR1ns2q"},"outputs":[],"source":["from PIL import Image \n","import PIL\n","import os \n","from IPython.display import display\n","from IPython.display import Image as _Imgdis\n","# creating an object  \n","\n","print(\"Working with {0} images\".format((len(class_subset))))\n","\n","for i in class_subset[:5]:\n","    print(train_dir+i)\n","    display(_Imgdis(filename=train_dir + i, width=128, height=128))\n","for i in class_subset_prime[:5]:\n","    print(val_dir+i)\n","    display(_Imgdis(filename=val_dir + i, width=128, height=128))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfOWTcpydjW3"},"outputs":[],"source":["vgg = VGG16(input_shape=IMAGE_SIZE + [3], include_top=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtL-4CYSeAMt"},"outputs":[],"source":["vgg.input\n","     "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9AjrowveysQ"},"outputs":[],"source":["\n","\n","for layer in vgg.layers:\n","  layer.trainable = False\n","     \n","train_path='./Cardiomegaly/train'\n","test_path='./Cardiomegaly/val'\n","\n","folders = glob(train_path+\"/*\")\n","print(len(folders))\n","     \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cz-KEMaI7kgB"},"outputs":[],"source":["\n","x = Flatten()(vgg.output)\n","prediction = Dense(len(folders), activation='softmax')(x)\n","model_vgg = Model(inputs=vgg.input, outputs=prediction)\n","model_vgg.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6VI1xOB9MZ1"},"outputs":[],"source":["\n","from keras import optimizers\n","\n","\n","adam = optimizers.Adam()\n","model_vgg.compile(loss='binary_crossentropy',\n","              optimizer=adam,\n","              metrics=['accuracy','AUC'])\n","     \n","\n","train_datagen = ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest')\n","     \n","\n","test_datagen = ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest')\n","\n","\n","\n","\n","train_set = train_datagen.flow_from_directory(train_path,\n","                                                 target_size = (128, 128),\n","                                                 batch_size = 32,\n","                                                 class_mode = 'categorical')\n","\n","\n","\n","test_set = test_datagen.flow_from_directory(test_path,\n","                                            target_size = (128, 128),\n","                                            batch_size = 32,\n","                                            class_mode = 'categorical')\n","     \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRsfRLVO9QSs"},"outputs":[],"source":["from datetime import datetime\n","from keras.callbacks import ModelCheckpoint\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","checkpoint = ModelCheckpoint(filepath='./Cardiomegaly/cardio.h5', \n","                               monitor='val_accuracy',verbose=2, save_best_only=True)\n","\n","callbacks = [checkpoint]\n","\n","start = datetime.now()\n","\n","model_history=model_vgg.fit_generator(\n","  train_set,\n","  validation_data=test_set,\n","  epochs=100,\n","  steps_per_epoch=1,\n","  validation_steps=1,\n","    callbacks=callbacks ,verbose=2)\n","\n","\n","duration = datetime.now() - start\n","print(\"Training completed in time: \", duration)\n"]},{"cell_type":"code","source":["\n","\n","_# Plot training & validation loss values\n","plt.plot(model_history.history['accuracy'])\n","plt.plot(model_history.history['val_accuracy'])\n","plt.title('CNN Model accuracy values')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","     \n"],"metadata":{"id":"nbqOOeQJvguh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_Yh0KVjVqrS"},"source":["__Your Answer:__\n","\n","\n","\n","\n","Highest validation accuracy was 0.85714, yet it was not a consistent result. The dataset seems to be little for this purpose. The metrics are defined as follows: \n","val_loss: 1.4717 - val_accuracy: 0.8571 - val_auc: 0.8367 "]},{"cell_type":"markdown","metadata":{"id":"nsnPBMwIVsho"},"source":["### Question 4.2 (10 points)\n","What is your best validation accuracy of fine-tuning a 16-layer VGGNet WITH ImageNet weights? In your model with the lowest validation loss, what are the hyperparameters? What is the validation loss? What is the training loss/accuracy? What are the pros and cons of using the pre-trained weights? Word limit: 150 words. Please include your code in the assignment submission."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKNqvD_9Vt8e"},"outputs":[],"source":["#@title\n","## Your code goes here\n","################################################################################\n","vgg_Imagenet = VGG16(input_shape=IMAGE_SIZE + [3],weights='imagenet', include_top=False)\n","vgg_Imagenet.input\n","\n","for layer in vgg_Imagenet.layers:\n","  layer.trainable = False\n","     \n","train_path='./Cardiomegaly/train'\n","test_path='./Cardiomegaly/val'\n","\n","folders = glob(train_path+\"/*\")\n","print(len(folders))\n","\n","\n","x = Flatten()(vgg_Imagenet.output)\n","prediction = Dense(len(folders), activation='softmax')(x)\n","model_vgg_Imagenet = Model(inputs=vgg_Imagenet.input, outputs=prediction)\n","model_vgg_Imagenet.summary()\n","\n","from keras import optimizers\n","\n","\n","adam = optimizers.Adam()\n","model_vgg_Imagenet.compile(loss='binary_crossentropy',\n","              optimizer=adam,\n","              metrics=['accuracy','AUC'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vu3CVI7CnAD"},"outputs":[],"source":["from datetime import datetime\n","from keras.callbacks import ModelCheckpoint\n","\n","\n","\n","checkpoint = ModelCheckpoint(filepath='./Cardiomegaly/cardio_Imagenet.h5', \n","                               verbose=2, save_best_only=True,monitor='val_accuracy')\n","\n","callbacks = [checkpoint]\n","\n","start = datetime.now()\n","\n","model_history=model_vgg_Imagenet.fit_generator(\n","  train_set,\n","  validation_data=test_set,\n","  epochs=100,\n","  steps_per_epoch=1,\n","  validation_steps=1,\n","    callbacks=callbacks ,verbose=2)\n","\n","\n","duration = datetime.now() - start\n","print(\"Training completed in time: \", duration)"]},{"cell_type":"code","source":["\n","\n","_# Plot training & validation loss values\n","plt.plot(model_history.history['accuracy'])\n","plt.plot(model_history.history['val_accuracy'])\n","plt.title('CNN Model accuracy values')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","     \n"],"metadata":{"id":"9Ms0QCuGvka6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-6mlL3eGklj3"},"source":["__Your Answer:__"]},{"cell_type":"markdown","metadata":{"id":"uP3BgLw-mVh_"},"source":["With the imagenet weights, the hyperparameters show a more even behavior, yet the biggest one was not better than the previous model without Imagenet weights. val_loss: 3.9508 - val_accuracy: 0.7143 - val_auc: 0.6327.\n","\n","\n","Using pre-trained models might help to speed up coding processes and tool development. But they might not be sensible to the kind of data we are trying to trained them with, since such models were trained with a different datasets originally."]},{"cell_type":"markdown","metadata":{"id":"USEauOuqVwMa"},"source":["## Question 5: Multi-class classification and the BMI707/EPI290 Kaggle contest (20 points)\n","In this question, we will build multi-class classifiers to distinguish three types of lung diseases using the NIHCXR8 data.\n","Please download the training set and the validation set from the BMI707/EPI290 Kaggle contest website (https://www.kaggle.com/c/2023bmi707-assignment-2-q5/data). Please note that this dataset is different from the one we used in Questions 3 and 4. Please DO NOT use any additional dataset (including those from NIH CXR8) to train or augment your models. Feel free to use any (ImageNet or any custom) architecture to classify all available classes. \n","\n","In your model with the lowest validation loss, what are the hyperparameters? What is the validation loss/accuracy? What is the training loss/accuracy? \n","\n","Please participate in the BMI707/EPI290 internal Kaggle contest (https://www.kaggle.com/c/2023bmi707-assignment-2-q5) and compare your results with others there. Your test set prediction performance evaluation will be evaluated instantaneously on Kaggle. An ensemble of models is allowed. The top 5 submissions with the highest accuracy on the private test set (testPrivate.tar) will receive bonus points. Please include your code in the assignment submission."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFvVk30iVyXL"},"outputs":[],"source":["## Your code goes here\n","################################################################################\n","!mkdir BMI707EPI290_Kaggle\n","#!wget  -O https://www.dropbox.com/s/feaakvusdvkep3n/train.tar ./BMI707EPI290_Kaggle/train.tar\n","#!wget  -O https://www.dropbox.com/s/9kebfecemhfkj7k/train.csv ./BMI707EPI290_Kaggle/train.csv\n","#!wget  -O https://www.dropbox.com/s/20d8b9z8znc2lmp/testPublic.tar ./BMI707EPI290_Kaggle/testPublic.tar\n","#!wget  -O https://www.dropbox.com/s/lun96wxiq84z7eg/testPrivate.tar ./BMI707EPI290_Kaggle/testPrivate.tar\n","#!wget  -O https://www.dropbox.com/s/a9o0lkszeusdmr9/sampleSubmission.csv ./BMI707EPI290_Kaggle/sampleSubmission.csv\n","\n","\n","!tar -xf ./BMI707EPI290_Kaggle/train.tar\n","!tar -xf ./BMI707EPI290_Kaggle/testPublic.tar\n","!tar -xf ./BMI707EPI290_Kaggle/testPrivate.tar\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["\n","# read in the labels\n","train_labels = pd.read_csv('./BMI707EPI290_Kaggle/train.csv',  header=None, index_col=0)\n","\n","# we will resize the images to make them smaller\n","# feel free to adjust this step in your assignment submission\n","IMAGE_SIZE = [128,128]\n","\n","\n","# read in the training images\n","train_images = []\n","train_dir = './BMI707EPI290_Kaggle/train/'\n","train_files = os.listdir(train_dir)\n","\n","\n","\n","\n","# reorder the labels so that they line up with the order of the image files\n","train_labels = train_labels.reindex(train_files)\n","\n","\n","\n","train_X=train_images\n","from keras.callbacks import EarlyStopping\n","from sklearn.preprocessing import LabelBinarizer\n","label_transformer = LabelBinarizer()\n","train_labels.fillna(0, inplace=True)\n","train_y = label_transformer.fit_transform(train_labels)\n","\n","\n","\n"],"metadata":{"id":"6QXqwdWUhX8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.3)\n","\n","datagen = ImageDataGenerator(brightness_range=[0.4,1.0])\n","\n","\n","\n","X_train=np.expand_dims(X_train, 3)\n","X_val=np.expand_dims(X_val, 3)\n","\n","X_train = tf.keras.applications.inception_v3.preprocess_input(X_train)\n","X_val = tf.keras.applications.inception_v3.preprocess_input(X_val)\n","\n","\n","train_generator = datagen.flow(X_train, y_train.tolist())\n","val_generator = datagen.flow(X_val, y_val.tolist())\n","epochs=30\n","cb_early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n","\n","\n"],"metadata":{"id":"aj8oCT_CGS03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","base_model = tf.keras.applications.InceptionV3(input_shape=IMAGE_SIZE+[3],include_top=False, weights='imagenet')\n","base_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","hist = base_model.fit_generator(train_generator,\n","                           epochs=epochs,\n","                           verbose=2,\n","                           steps_per_epoch = 230,\n","                           validation_data=val_generator,\n","                           validation_steps = 70,\n","                           callbacks=[cb_early_stopping])\n","\n","\n"],"metadata":{"id":"IiUMLzh7GVVV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MpVkb-gDV0Fj"},"source":["## Question 6: Limitations of the NIH CXR8 dataset and the road ahead (5 points)"]},{"cell_type":"markdown","metadata":{"id":"5jfRAuFuV0nH"},"source":["Please list three limitations of models trained from this dataset. Word limit: 150 words."]},{"cell_type":"markdown","metadata":{"id":"EGGjMU3aU_7H"},"source":["__Your Answer:__\n","\n","The dataset might not be overall consistent or an excellent representation of the pathology it studies, which means that the models might not achieve the best accuracy. Besides, the dataset contains more information in terms of finding labels. It would be perhaps interesting to train a more robust model including any of the categories that were excluded, so the models could perform differential diagnosis while extracting the features contained in the X-ray images. Nonetheless, this dataset might lack the right kind of images for pathologies such as Cardiomegaly to be diagnosed. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"fqK0h9zvWibE"},"source":["# Part 2: Recurrent Neural Networks (20 points)\n","\n","In this exercise, we will implement recurrent neural networks to classify newswires from Reuters. The descriptions of the dataset could be found at https://keras.io/datasets/#reuters-newswire-topics-classification. You will implement two recurrent neural network models: one with the simple recurrent neural network (SimpleRNN) function and the other uses long short-term memory (LSTM) or gated recurrent unit (GRU)."]},{"cell_type":"markdown","metadata":{"id":"i1GWJFuVWbfO"},"source":["## Question 7: Build two recurrent neural networks, one with SimpleRNN and the other with LSTM or GRU for text classification (15 points)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfYxSWOumpki"},"outputs":[],"source":["## Some starter codes\n","# load the required packages\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Activation, Dropout, Embedding, Input, Dropout, LSTM, GRU\n","from tensorflow.keras.datasets import reuters\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras import layers\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","max_features = 10000  # number of words to consider as features\n","maxlen = 500          # cut texts after this number of words\n","batch_size = 64\n","\n","# load data from keras.datasets\n","(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_features)\n","\n","# pad the sequences to make them of equal length\n","x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n","\n","# convert the categories to one-hot encoding\n","y_train = to_categorical(y_train, 46)\n","y_test = to_categorical(y_test, 46)\n","\n","\n","num_timesteps = x_train.shape[1]\n","\n","# let's define a sequential model, you would have to complete the model definition and start to train the models\n","modelLSTM = Sequential(\n","    [\n","      layers.Input(shape=(num_timesteps, )),\n","      layers.Embedding(input_dim = max_features+1, output_dim = 128, name='embedding'),\n","      layers.LSTM(units = 128),\n","      layers.Dropout(0.5),\n","      layers.Dense(256, activation='relu'),\n","      layers.Dropout(0.5),\n","      layers.Dense(units=46, activation='softmax', name='output')\n","  ]\n",")\n","modelSRNN = Sequential(\n","    [\n","      layers.Input(shape=(num_timesteps, )),\n","      layers.Embedding(input_dim = max_features+1, output_dim = 128, name='embedding'),\n","      layers.SimpleRNN(units = 128),\n","      layers.Dropout(0.5),\n","      layers.Dense(256, activation='relu'),\n","      layers.Dropout(0.5),\n","      layers.Dense(units=46, activation='softmax', name='output')\n","    ]\n",")\n","## Your code goes here\n","################################################################################\n","\n","modelGRU = Sequential(\n","    [\n","      layers.Input(shape=(num_timesteps, )),\n","      layers.Embedding(input_dim = max_features+1, output_dim = 128, name='embedding'),\n","      layers.GRU(units = 128),\n","      layers.Dropout(0.5),\n","      layers.Dense(256, activation='relu'),\n","      layers.Dropout(0.5),\n","      layers.Dense(units=46, activation='softmax', name='output')\n","    ]\n",")\n","\n"]},{"cell_type":"code","source":["modelLSTM.summary()\n","modelLSTM.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy','AUC'])\n","modelLSTM.fit(x_train,y_train,epochs=10, validation_split=0.2, batch_size = 64)\n"],"metadata":{"id":"J1cM8tEMM7Bu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelSRNN.summary()\n","modelSRNN.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy','AUC'])\n","modelSRNN.fit(x_train,y_train,epochs=10, validation_split=0.2, batch_size =  64)\n"],"metadata":{"id":"wkhtZ6p6ZUlo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelGRU.summary()\n","modelGRU.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy','AUC'])\n","modelGRU.fit(x_train,y_train,epochs=10, validation_split=0.2, batch_size =  64)"],"metadata":{"id":"upfGCtECFaYL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"LSTM Results\")\n","modelLSTM.evaluate(x_test,y_test)\n","\n","\n","print(\"SRNN Results\")\n","modelSRNN.evaluate(x_test,y_test)\n","\n","\n","print(\"GRU Results\")\n","modelGRU.evaluate(x_test,y_test)"],"metadata":{"id":"GypuC-_PZb7H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NMeBxJtnxH0"},"source":["## Question 8: How does the performance of your models compare? Is the performance difference (if any) expected? (5 points)\n","\n","Word limit: 100 words."]},{"cell_type":"markdown","metadata":{"id":"mmNA0FTQky06"},"source":["__Your Answer:__\n","\n","\n","\n","Based on literature, LSTMs have demonstrated to achieve better results than the Simple RNNs. In our little experiment, that is easy to see, since any of the hyperparameters have reached considerably better values in one model, even though they have almost the same architecture, despite the RNN layer. Nonetheless, LSTMs possess a more robust algorithm and therefore they take longer when training.\n","\n","\n","LSTM Results\n","71/71 [==============================] - 19s 265ms/step - loss: 2.1266 - accuracy: 0.6224 - auc: 0.9053\n","SRNN Results\n","71/71 [==============================] - 3s 48ms/step - loss: 2.2049 - accuracy: 0.4337 - auc: 0.9034\n","\n","[2.2049343585968018, 0.4336598515510559, 0.9034375548362732]"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1Yc2eFOQk9fmkAbH10cL3X7fbr5hb_Jdp","timestamp":1681877979055},{"file_id":"135ohQc7CDbv4wbSvmj2hkrjsDFcAfHIw","timestamp":1554685197222}],"private_outputs":true,"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}