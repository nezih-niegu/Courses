{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94db6f5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1Z3JvAFmL2IkBnQmmt5f4uTcXVhO5f7cq\"/></center>\n",
    "\n",
    "------\n",
    "<center>&copy; Research Group CAMMA, University of Strasbourg, <a href=\"http://camma.u-strasbg.fr\">http://camma.u-strasbg.fr</a> \n",
    "\n",
    "<h2>Author: Vinkle Srivastav </h2>\n",
    "</center>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211a8e8",
   "metadata": {},
   "source": [
    "# <center><font color=green> Lecture 6: Surgical Workflow Recognition using PyTorch </font></center>\n",
    "<center><img src=\"https://drive.google.com/uc?id=1M9UPbnuvU8VTQ3_QGVpzdCE-5kUgv-2U\"/></center>\n",
    "\n",
    "\n",
    "### **Objectives**: \n",
    "  1. PyTorch `Dataset` and `Dataloader` for subset of cholec80\n",
    "  2. Visualize sample cholec80 images using PyTorch dataloaders\n",
    "  3. Develop the surgical phase classification model\n",
    "  4. Extract the features for the \"train\" and \"val\" dataset\n",
    "  5. Train the model on extracted features\n",
    "  6. Perform online inference on a sample cholec80 surgical video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbebe13",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1fb4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/nezih-niegu/.local/lib/python3.8/site-packages (1.22.4)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1752, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1390, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/segment.py\", line 245, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1368, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/home/nezih-niegu/.local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 148, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 237, in pip_self_version_check\n",
      "    logger.info(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.1.2', new='22.2.1'),)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (3.1.2)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1752, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1390, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/segment.py\", line 245, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1368, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/home/nezih-niegu/.local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 148, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 237, in pip_self_version_check\n",
      "    logger.info(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.1.2', new='22.2.1'),)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/nezih-niegu/.local/lib/python3.8/site-packages (1.12.0)\n",
      "Requirement already satisfied: typing-extensions in /home/nezih-niegu/.local/lib/python3.8/site-packages (from torch) (4.2.0)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1752, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1390, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/segment.py\", line 245, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1368, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/home/nezih-niegu/.local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 148, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 237, in pip_self_version_check\n",
      "    logger.info(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.1.2', new='22.2.1'),)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in /home/nezih-niegu/.local/lib/python3.8/site-packages (0.13.0)\n",
      "Requirement already satisfied: torch==1.12.0 in /home/nezih-niegu/.local/lib/python3.8/site-packages (from torchvision) (1.12.0)\n",
      "Requirement already satisfied: numpy in /home/nezih-niegu/.local/lib/python3.8/site-packages (from torchvision) (1.22.4)\n",
      "Requirement already satisfied: typing-extensions in /home/nezih-niegu/.local/lib/python3.8/site-packages (from torchvision) (4.2.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision) (2.22.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/nezih-niegu/.local/lib/python3.8/site-packages (from torchvision) (9.2.0)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1752, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1390, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/segment.py\", line 245, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1368, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/home/nezih-niegu/.local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 148, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 237, in pip_self_version_check\n",
      "    logger.info(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.1.2', new='22.2.1'),)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/nezih-niegu/.local/lib/python3.8/site-packages (4.64.0)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1752, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1390, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/segment.py\", line 245, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1368, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/home/nezih-niegu/.local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 148, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 237, in pip_self_version_check\n",
      "    logger.info(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.1.2', new='22.2.1'),)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipywidgets in /usr/lib/python3/dist-packages (6.0.0)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1752, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1390, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/segment.py\", line 245, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1368, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/home/nezih-niegu/.local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 148, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 237, in pip_self_version_check\n",
      "    logger.info(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/nezih-niegu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.1.2', new='22.2.1'),)\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install tqdm\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d88d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download resources\n",
    "DIR=\"./resources\"\n",
    "![ ! -d \"$DIR\" ] && wget https://s3.unistra.fr/camma_public/teaching/edu4sds_resources/lec6_surg-workflow/resources.zip && unzip -qq resources.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6c35b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'resources/cholec80/ch80_5vids_train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-606dd3ccabcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# others\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mMETA_DATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_JSON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# we use two transformations for our input: transforms.ToTensor() converts images loaded by Pillow into PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resources/cholec80/ch80_5vids_train.json'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import ipywidgets as wd\n",
    "import glob\n",
    "import io\n",
    "\n",
    "# paths\n",
    "ROOT_DIR = \"resources/cholec80\"\n",
    "TRAIN_JSON = os.path.join(ROOT_DIR, \"ch80_5vids_train.json\")\n",
    "VAL_JSON = os.path.join(ROOT_DIR, \"ch80_3vids_val.json\")\n",
    "FEATURES_PATH = os.path.join(ROOT_DIR, \"ch80_train_val_features.pkl\")\n",
    "PRETRAINED_MODEL_PATH = os.path.join(ROOT_DIR, \"resnet50_ch80_20p_ft.pth\")\n",
    "FINAL_MODEL_PATH = os.path.join(ROOT_DIR, \"resnet50_ch80_surg-flow.pth\")\n",
    "FINAL_LOG_PATH = os.path.join(ROOT_DIR, \"train_log.json\")\n",
    "DO_TRAINING = True\n",
    "\n",
    "# learning parameters\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "CHANNEL_DIMS = [2048, 128]\n",
    "POS_WEIGHTS_PHASE = [1.92, 0.20, 0.99, 0.30, 1.94, 1.0, 2.18]\n",
    "LEARNING_RATE = 0.003\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "MILE_STONES = [150, 180]\n",
    "LR_GAMMA = 0.33\n",
    "\n",
    "# others\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "META_DATA = json.load(open(TRAIN_JSON))[\"metadata\"]\n",
    "\n",
    "# we use two transformations for our input: transforms.ToTensor() converts images loaded by Pillow into PyTorch tensors \n",
    "# and transforms.Normalize() adjusts the values of the tensor so that their average is zero and their standard deviation is 0.5. \n",
    "IM2TENSOR = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"using device={}, PyTorch version={}, torchvision version={}\".format(\n",
    "        DEVICE, torch.__version__, torchvision.__version__\n",
    "    )\n",
    ")\n",
    "META_DATA[\"phases\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff09a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to conver PIL image to byte array\n",
    "def image_to_byte_array(image):\n",
    "  imgByteArr = io.BytesIO()\n",
    "  image.save(imgByteArr, format=image.format)\n",
    "  imgByteArr = imgByteArr.getvalue()\n",
    "  return imgByteArr\n",
    "\n",
    "# helper function to compute the accuracy from the ground truth labels and model predictions\n",
    "def accuracy(labels, predictions):\n",
    "    t, predicted = torch.max(predictions, 1)\n",
    "    acc = (predicted == labels).float()\n",
    "    return acc.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8dd389",
   "metadata": {},
   "source": [
    "## Cholec80 dataset\n",
    "Cholec80 dataset [1] consists of 80 videos of the cholecystectomy procedures. The videos are captured at 25 fps with a resolution of 854 × 480 or 1920 × 1080. This tutorial uses a subset of the cholec80 dataset containing 5 videos for training and 3 videos for validation downsampled at 1 fps with a spatial resolution of 399x224. The surgical phases consist of 7 labels annotated by expert surgeons, as shown below.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1BHfCt8Obh1iTaeMSkpt9yDq1_-WIfSbG\"/></center>\n",
    "\n",
    "\n",
    "_1. Twinanda, Andru P., Sherif Shehata, Didier Mutter, Jacques Marescaux, Michel De Mathelin, and Nicolas Padoy. \"Endonet: a deep architecture for recognition tasks on laparoscopic videos.\" IEEE transactions on medical imaging 36, no. 1 (2016): 86-97._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dec726",
   "metadata": {},
   "source": [
    "## 1. PyTorch `Dataset` and `Dataloader` for Cholec80\n",
    "PyTorch uses two basic primitives to handle the data: a dataset object using `torch.utils.data.Dataset` to store the samples and their corresponding labels, and a dataloader object using `torch.utils.data.DataLoader` to wrap the dataset object to easy access the samples of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68fd79e",
   "metadata": {},
   "source": [
    "### PyTorch `Dataset`\n",
    "Now, let's go through the details of how to set the dataset class by extending `torch.utils.data.Dataset`. First, we will write the initialization function (`__init__()`)  of the class to read the ground truth labels and/or features of the \"train\" and the \"val\" set. The `__getitem__()` function will be used to read the image/feature along with the ground-truth phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3823869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CholecDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    CholecDataset class to give batch of images from the subset of cholec80 dataset (http://camma.u-strasbg.fr/datasets)\n",
    "    Arguments:\n",
    "        gt_json {str} -- path to json file containing ground truth annotations\n",
    "        data_split {str} -- \"train\" or \"val\" split\n",
    "        features [{dict}]-- list of dictionary containing \"file_name\" and 2048 dimensional resnet layer5 features\n",
    "    \"\"\" \n",
    "    def __init__(\n",
    "        self,\n",
    "        gt_json=\"\",\n",
    "        data_split=\"train\",\n",
    "        features=None,\n",
    "        extract_features=False,\n",
    "    ):\n",
    "        self.gt_json = gt_json\n",
    "        self.data_split = data_split\n",
    "        self.root_dir = ROOT_DIR\n",
    "        data = json.load(open(gt_json))\n",
    "        self.meta_data = data[\"metadata\"]\n",
    "        self.anns = data[\"annotations\"]\n",
    "        self.extract_features = extract_features\n",
    "        self.transform = IM2TENSOR\n",
    "        if features is not None:\n",
    "            features_dict = {f[\"file_name\"]: f[\"features\"] for f in features}\n",
    "            self.anns = [\n",
    "                dict(ann, features=features_dict[ann[\"file_name\"]])\n",
    "                for ann in self.anns\n",
    "            ]\n",
    "        print(\"=> Dataset loaded for {}\".format(str(self), self.meta_data))\n",
    "    def __len__(self):\n",
    "        # give the length of the datasets\n",
    "        return len(self.anns)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # print the datasets\n",
    "        return \"CholecDataset(\" + self.data_split + \")\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # if the mode is feature extraction return the PIL image and the file_name\n",
    "        if self.extract_features:\n",
    "            img_path = os.path.join(\n",
    "                self.root_dir, self.data_split, self.anns[idx][\"file_name\"]\n",
    "            )\n",
    "            image = Image.open(img_path)\n",
    "            image = self.transform(image)\n",
    "            return image, self.anns[idx][\"file_name\"], self.anns[idx][\"phase\"]\n",
    "        # else return the features of size 1x2048 and the corresponding annotations for phase\n",
    "        else:\n",
    "            features = torch.from_numpy(np.array(self.anns[idx][\"features\"]))\n",
    "            phase_gt = self.anns[idx][\"phase\"]\n",
    "            return features, phase_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7f3f38",
   "metadata": {},
   "source": [
    "#### PyTorch `dataloaders` for \"train\" and \"val\" set\n",
    "Now, we will use the `torch.utils.data.DataLoader`, which wraps around the dataset class and uses multi-processing to provide a batch of sampled data. The `torch.utils.data.DataLoader` takes dataset object `torch.utils.data.Dataset`, `batch_size`, and `shuffle` as input. We have defined the dataset object above. The `batch_size` and `shuffle` parameters are described below. \n",
    "\n",
    "1. `batch_size` denotes the number of samples contained in each generated batch.\n",
    "2. `shuffle` - If set to True, we will get a random order of samples from the dataset at each pass. Shuffling the order of examples during training helps to make our model more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "801cf1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(\n",
    "    extract_features=False, features=None, shuffle=False, batch_size=BATCH_SIZE\n",
    "):\n",
    "    \"\"\"\n",
    "    get the Pytorch dataloaders for the the \"train\" and the \"val\" set\n",
    "    Arguments:\n",
    "        extract_features {bool} -- whether to use the dataset in the feature extraction mode\n",
    "        features {dict} -- resnet last layer features for the \"train\" or \"val\" split\n",
    "        shuffle {bool} -- whether to shuffle the training dataset\n",
    "        batch_size {int} -- batch size for the training\n",
    "    Return:\n",
    "        train and val dataloasers\n",
    "    \"\"\"    \n",
    "    train_dataset = CholecDataset(\n",
    "        TRAIN_JSON,\n",
    "        data_split=\"train\",\n",
    "        features=features[\"train\"] if features is not None else None,\n",
    "        extract_features=extract_features,\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    val_dataset = CholecDataset(\n",
    "        VAL_JSON,\n",
    "        data_split=\"val\",\n",
    "        features=features[\"val\"] if features is not None else None,\n",
    "        extract_features=extract_features,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df629272",
   "metadata": {},
   "source": [
    "## 2. Let's visualize some images by iterating through dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8db4e86",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'resources/cholec80/ch80_5vids_train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1e97699265eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# call the function to visualize the images (execute multiple times to see different images from the \"train\" set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mvisualize_cholec80_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-1e97699265eb>\u001b[0m in \u001b[0;36mvisualize_cholec80_images\u001b[0;34m(num_images)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_cholec80_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_loader_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphases_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-8cf8dfa186ce>\u001b[0m in \u001b[0;36mget_dataloaders\u001b[0;34m(extract_features, features, shuffle, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mval\u001b[0m \u001b[0mdataloasers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"    \n\u001b[0;32m---> 14\u001b[0;31m     train_dataset = CholecDataset(\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mTRAIN_JSON\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdata_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-103ff9947747>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gt_json, data_split, features, extract_features)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mROOT_DIR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"annotations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resources/cholec80/ch80_5vids_train.json'"
     ]
    }
   ],
   "source": [
    "def visualize_cholec80_images(num_images = 4):\n",
    "    train_loader_feats, _ = get_dataloaders(extract_features=True, shuffle=True)\n",
    "    image_batch, file_names, phases_id = next(iter(train_loader_feats))\n",
    "    print(image_batch.shape)\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        # take the ithe image, and convert from CxHxW to HxWxC\n",
    "        img = image_batch[i].cpu().squeeze().numpy().transpose(1, 2, 0)\n",
    "        # rescale the image in the range 0 to 1\n",
    "        vmin, vmax = img.min(), img.max()\n",
    "        img = (img - vmin) / (vmax - vmin)\n",
    "        # show the image\n",
    "        plt.imshow(img)\n",
    "        title = META_DATA[\"phases\"][phases_id[i]] \n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.grid()\n",
    "    plt.gcf().tight_layout()\n",
    "    \n",
    "# call the function to visualize the images (execute multiple times to see different images from the \"train\" set)\n",
    "visualize_cholec80_images()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080982b",
   "metadata": {},
   "source": [
    "### 3. Surgical phase classification model\n",
    "In the following, we will develop a classification model for surgical workflow recognition. We first load the resnet50 model weights in the `feature_extractor` module that are trained on the subset of the cholec80 dataset. Then, we add a few fully connected layers in the `phase_fc` module to perform the classification. PyTorch's modular design allows independently using the `feature_extractor` and `phase_fc`.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1z_SpM23Ha2E1zDGzg9T9ImhBt82Yjh0K\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c7d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CholecModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple classification model for the surgical workflow recognition.\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        super(CholecModel, self).__init__()\n",
    "        if PRETRAINED_MODEL_PATH:\n",
    "            model = models.resnet50()\n",
    "            print(\"=> loading backbone weights\")\n",
    "            m, v = model.load_state_dict(\n",
    "                torch.load(PRETRAINED_MODEL_PATH), strict=False\n",
    "            )\n",
    "            print(\"=> backbone weights loaded... \\nmissing keys = {}  invalid keys {}\".format(m, v))\n",
    "        else:\n",
    "            # load the model with imagenet weights\n",
    "            model = models.resnet50(pretrained=True)\n",
    "            \n",
    "        # feature_extractor module to extract the feature from a given image\n",
    "        self.feature_extractor = torch.nn.Sequential(\n",
    "            *(list(model.children())[:-1])\n",
    "        )\n",
    "        \n",
    "        # develop layers for the classification module\n",
    "        last_dim = CHANNEL_DIMS[0]\n",
    "        layers = []\n",
    "        for i, dim in enumerate(CHANNEL_DIMS[1:]):\n",
    "            layers.append(nn.Linear(last_dim, dim, bias=False))\n",
    "            layers.append(nn.BatchNorm1d(dim, eps=1e-05, momentum=0.1))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(p=0.9))\n",
    "            last_dim = dim\n",
    "        # add the last layer for the phase classification\n",
    "        layers.append(\n",
    "            nn.Linear(CHANNEL_DIMS[-1], len(META_DATA[\"phases\"]), bias=True)\n",
    "        )\n",
    "        # phase_fc module to perform the classification\n",
    "        self.phase_fc = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the features from the image\n",
    "        features = self.feature_extractor(x).flatten(1)\n",
    "        # get the classification logits\n",
    "        phase_logits = self.phase_fc(features)\n",
    "        return phase_logits\n",
    "print(CholecModel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791832d1",
   "metadata": {},
   "source": [
    "## Two-stage training\n",
    "We will do the training in two stages: in the first stage, we will extract the feature for the \"train\" and the \"val\" set using `feature_extractor` module, and in the stage, we will only train the `phase_fc` module using the extracted features. The two-stage model helps train the model faster and develop a robust model based on temporal information from the previous frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46be445",
   "metadata": {},
   "source": [
    "### 4. Extract the features for the \"train\" and \"val\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be8b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage 1: feature extraction\n",
    "def extract_features(save=True):\n",
    "    if os.path.isfile(FEATURES_PATH):\n",
    "        print(\"loading features from : {}\".format(FEATURES_PATH))\n",
    "        features_all = pickle.load(open(FEATURES_PATH, \"rb\"))\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            model = CholecModel().to(DEVICE).eval()\n",
    "            features_all = {}\n",
    "            train_loader_feats, val_loader_feats = get_dataloaders(\n",
    "                extract_features=True\n",
    "            )\n",
    "            loaders = [train_loader_feats, val_loader_feats]\n",
    "            names = [\"train\", \"val\"]\n",
    "            for loader, name in zip(loaders, names):\n",
    "                print(\"\\nextracting features for the \", str(loader.dataset))\n",
    "                features = []\n",
    "                for image_batch, filenames, _ in tqdm(loader):\n",
    "                    image_batch = image_batch.to(DEVICE)\n",
    "                    feats = (\n",
    "                        model.feature_extractor(image_batch)\n",
    "                        .squeeze()\n",
    "                        .cpu()\n",
    "                        .numpy()\n",
    "                    )\n",
    "                    features += [\n",
    "                        {\"file_name\": name, \"features\": f}\n",
    "                        for f, name in zip(feats, filenames)\n",
    "                    ]\n",
    "                features_all[name] = features\n",
    "            with open(FEATURES_PATH, \"wb\") as f:\n",
    "                pickle.dump(features_all, f)\n",
    "                print(\"saving features at : {}\".format(FEATURES_PATH))\n",
    "    return features_all\n",
    "\n",
    "# call the function to extract the features\n",
    "features = extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81447acc",
   "metadata": {},
   "source": [
    "## 5. Train the model on extracted features\n",
    "\n",
    "### a) Get the train and val dataloaders b) define the loss function c) get the model object d) define optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf226ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the loaders initialized from the extracted features\n",
    "train_loader, val_loader = get_dataloaders(features=features, shuffle=True)\n",
    "\n",
    "# Define the cross entropy loss.\n",
    "loss_phase_fn = nn.CrossEntropyLoss(weight=torch.FloatTensor(POS_WEIGHTS_PHASE).to(DEVICE), ignore_index=-1)\n",
    "\n",
    "# define the optimizer. Since we only need to train phase_fc module. We will \n",
    "cholec_model = CholecModel().to(DEVICE)\n",
    "cholec_model.feature_extractor.eval()\n",
    "for params in cholec_model.feature_extractor.parameters():\n",
    "    params.requires_grad = False\n",
    "parameters = filter(lambda p: p.requires_grad, cholec_model.parameters())\n",
    "optimizer = optim.SGD(\n",
    "    parameters,\n",
    "    lr=LEARNING_RATE,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    nesterov=True,\n",
    ")\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer, milestones=MILE_STONES, gamma=LR_GAMMA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbda06",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe3fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_TRAINING:\n",
    "    pbar = tqdm(range(NUM_EPOCHS))\n",
    "    train_logs = []\n",
    "    for epoch in pbar:\n",
    "        train_loss, val_loss, acc_metric_train, acc_metric_val = 0.0, 0.0, 0.0, 0.0\n",
    "        # training epoch\n",
    "        cholec_model.phase_fc.train()\n",
    "        for feats, phase_gt in train_loader:\n",
    "            feats, phase_gt = feats.to(DEVICE), phase_gt.to(DEVICE)\n",
    "            phase_logits = cholec_model.phase_fc(feats)\n",
    "            loss = loss_phase_fn(phase_logits, phase_gt)\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            acc_metric_train += accuracy(phase_gt, phase_logits)    \n",
    "        train_loss /= len(train_loader)\n",
    "        acc_metric_train = 100 * acc_metric_train / len(train_loader)\n",
    "        \n",
    "        # validation epoch\n",
    "        with torch.no_grad():\n",
    "            cholec_model.phase_fc.eval()\n",
    "            for feats, phase_gt in val_loader:\n",
    "                feats, phase_gt = feats.to(DEVICE), phase_gt.to(DEVICE)\n",
    "                phase_logits = cholec_model.phase_fc(feats)\n",
    "                loss = loss_phase_fn(phase_logits, phase_gt)\n",
    "                val_loss += loss.item()\n",
    "                acc_metric_val += accuracy(phase_gt, phase_logits)\n",
    "            val_loss /= len(val_loader)\n",
    "            acc_metric_val = 100 * acc_metric_val / len(val_loader)\n",
    "        \n",
    "        # log and display\n",
    "        train_logs.append({'train_loss':train_loss, \n",
    "                        'val_loss':val_loss,\n",
    "                        'acc_metric_train':acc_metric_train,\n",
    "                        'acc_metric_val':acc_metric_val})\n",
    "        pbar.set_description(\n",
    "            \"train_loss: {:.3f} val_loss: {:.3f} train accuracy: {:.3f} val accuracy: {:.3f}\".format(\n",
    "                train_loss, val_loss, acc_metric_train, acc_metric_val\n",
    "            )\n",
    "        )\n",
    "    # save the trained model and logs\n",
    "    torch.save(cholec_model.state_dict(), FINAL_MODEL_PATH)\n",
    "    json.dump(train_logs, open(FINAL_LOG_PATH, \"w\"))\n",
    "else:\n",
    "    if os.path.isfile(FINAL_MODEL_PATH):\n",
    "        m,v = cholec_model.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=DEVICE))\n",
    "        print(\"=> loaded model weights from {} \\nmissing keys = {}  invalid keys {}\".format(FINAL_MODEL_PATH, m, v))\n",
    "        train_logs = json.load(open(FINAL_LOG_PATH))\n",
    "    else:\n",
    "        print(\"=> No model weights found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fefeb4",
   "metadata": {},
   "source": [
    "### Plot training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e822f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = [h['train_loss'] for h in train_logs]\n",
    "loss_val = [h['val_loss'] for h in train_logs]\n",
    "accuracy_train = [h['acc_metric_train'] for h in train_logs]\n",
    "accuracy_val = [h['acc_metric_val'] for h in train_logs]\n",
    "num_epochs = len(loss_train)\n",
    "print(len(train_logs))\n",
    "plt.figure(figsize=(12, 6))\n",
    "titles = [\"Accuracy vs. Number of Training Epochs\", \"Loss vs. Number of Training Epochs\"]\n",
    "ylabels = [\"Accuracy\", \"Loss\"]\n",
    "yplots = [(accuracy_train, accuracy_val), (loss_train, loss_val)]\n",
    "for i, title, ylabel, yplot in zip(range(2), titles, ylabels, yplots):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training Epochs\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.plot(range(1,num_epochs+1),yplot[0],label=\"Train-\"+ylabel)\n",
    "    plt.plot(range(1,num_epochs+1),yplot[1],label=\"Validation-\"+ylabel)\n",
    "    #plt.ylim((0.3,1.))\n",
    "    #plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "    plt.legend()\n",
    "plt.gcf().tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b478ef9",
   "metadata": {},
   "source": [
    "## 6. Perform live inference on a sample cholec80 surgical video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60e5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the video\n",
    "VIDEO_PATH_INFERENCE = os.path.join(ROOT_DIR, \"val/video41\") # or video41 or video42\n",
    "# read the paths of the video frames and sort them to make it sequential\n",
    "video_frames = sorted(\n",
    "    [\n",
    "        int(os.path.basename(a).replace(\".jpg\", \"\"))\n",
    "        for a in glob.glob(VIDEO_PATH_INFERENCE + \"/*.jpg\")\n",
    "    ]\n",
    ")\n",
    "video_frames = [os.path.join(VIDEO_PATH_INFERENCE, str(i) + \".jpg\") for i in video_frames]\n",
    "\n",
    "# read the ground truth file for checking the corresponding ground truth labels\n",
    "VAL_GT = {\n",
    "    os.path.join(ROOT_DIR, \"val\", p[\"file_name\"]): p[\"phase\"]\n",
    "    for p in json.load(open(VAL_JSON))[\"annotations\"]\n",
    "}\n",
    "test_image = open(video_frames[0], \"rb\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca3eba",
   "metadata": {},
   "source": [
    "### GUI using ipywidgets\n",
    "\n",
    "We will use `ipywidgets` to develop the graphical user interface (GUI) buttons to `play`, `pause`, `stop`, and `slide` the video. We will also add two text boxes to show the output of the model and the corresponding ground truth phase label.  We will use the model in the inference mode. \n",
    "\n",
    "We will define a callback function `slider_update` on the change of the slider value. Whenever slider value gets updated, we will read the corresponding frame, pass the image to the model, get the model prediction. Finally, we will display the image, model prediction, and the ground truth label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a9b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slider to scroll through the video\n",
    "slider = wd.IntSlider(value=0, min=0, max=len(video_frames) - 1)\n",
    "# play button to plat the video\n",
    "play_button = wd.Play(\n",
    "    value=0, min=0, max=len(video_frames) - 1, step=1, interval=1000\n",
    ")\n",
    "# text box to show the ground truth phase label\n",
    "gt_label = wd.Textarea(\n",
    "    value=\"ground truth: prepration\",\n",
    "    placeholder=\"\",\n",
    "    description=\"\",\n",
    "    disabled=False,\n",
    ")\n",
    "# text box to show the model prediction\n",
    "pred_label = wd.Textarea(\n",
    "    value=\"prediction: prepration\",\n",
    "    placeholder=\"\",\n",
    "    description=\"\",\n",
    "    disabled=False,\n",
    ")\n",
    "# image widget to show the image\n",
    "image_wd = wd.Image(value=test_image, width=600, height=336)\n",
    "# link the output of the play button to the slider\n",
    "wd.jslink((play_button, \"value\"), (slider, \"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model in the inference mode\n",
    "cholec_model.eval()\n",
    "def slider_update(change):\n",
    "    file_name = video_frames[change.new]\n",
    "    input_image = Image.open(file_name)\n",
    "    image_wd.value = image_to_byte_array(input_image)\n",
    "    with torch.no_grad():\n",
    "        image = IM2TENSOR(input_image)[None].to(DEVICE)\n",
    "        logits = cholec_model(image)\n",
    "    predicted_phase = META_DATA[\"phases\"][logits.argmax().item()]\n",
    "    gt_phase = META_DATA[\"phases\"][VAL_GT[file_name]]\n",
    "    gt_label.value = \"ground truth: \" + gt_phase\n",
    "    pred_label.value = \"prediction: \" + predicted_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3949bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the app\n",
    "slider.observe(slider_update, \"value\")\n",
    "out = wd.Output()\n",
    "app = wd.HBox(\n",
    "    [\n",
    "        wd.VBox([image_wd, wd.HBox([play_button, slider])]),\n",
    "        wd.VBox([gt_label, pred_label]),\n",
    "    ]\n",
    ")\n",
    "display(app)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
