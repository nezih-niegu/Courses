{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fef3b73",
   "metadata": {},
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1Z3JvAFmL2IkBnQmmt5f4uTcXVhO5f7cq\"/></center>\n",
    "\n",
    "------\n",
    "<center>&copy; Research Group CAMMA, University of Strasbourg, <a href=\"http://camma.u-strasbg.fr\">http://camma.u-strasbg.fr</a> \n",
    "\n",
    "<h2>Author: Deepak Alapatt </h2>\n",
    "</center>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90612350",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "# Learning Objective\n",
    "# <center><font color=green> Lecture 7: Surgical Semantic segmentation </font></center>\n",
    "<center><img src=\"https://drive.google.com/uc?id=1MrPylzmD6QIWcpe5pcadee00eeEH8GyP\"/></center>\n",
    "\n",
    "\n",
    "### **Objectives**: \n",
    "  1. PyTorch `Dataset` and `Dataloader` for a segmentation dataset\n",
    "  3. Develop the surgical segmentation model for model\n",
    "  5. Train the model to segment tool and anatomy instance on laparoscopic cholecystectomy frames\n",
    "  6. Perform online inference on a sample cholec80 surgical video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9d2a8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4d67fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install torch\n",
    "# !pip install torchvision\n",
    "# !pip install tqdm\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24489d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-21 19:54:35--  https://s3.unistra.fr/camma_public/teaching/edu4sds_resources/lec7_dl-segm/resources.zip\n",
      "Resolving s3.unistra.fr (s3.unistra.fr)... 130.79.200.152\n",
      "Connecting to s3.unistra.fr (s3.unistra.fr)|130.79.200.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3344266745 (3.1G) [application/zip]\n",
      "Saving to: ‘resources.zip’\n",
      "\n",
      "resources.zip        72%[=============>      ]   2.25G  4.07MB/s    in 5m 33s  \n",
      "\n",
      "2022-07-21 20:54:16 (6.94 MB/s) - Read error at byte 2420508136/3344266745 (Success). Retrying.\n",
      "\n",
      "--2022-07-21 20:54:17--  (try: 2)  https://s3.unistra.fr/camma_public/teaching/edu4sds_resources/lec7_dl-segm/resources.zip\n",
      "Connecting to s3.unistra.fr (s3.unistra.fr)|130.79.200.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 3344266745 (3.1G), 923758609 (881M) remaining [application/zip]\n",
      "Saving to: ‘resources.zip’\n",
      "\n",
      "resources.zip       100%[++++++++++++++=====>]   3.11G   960KB/s    in 8m 23s  \n",
      "\n",
      "2022-07-21 21:02:41 (1.75 MB/s) - ‘resources.zip’ saved [3344266745/3344266745]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download resources\n",
    "DIR=\"./resources\"\n",
    "![ ! -d \"$DIR\" ] && wget https://s3.unistra.fr/camma_public/teaching/edu4sds_resources/lec7_dl-segm/resources.zip && unzip -qq resources.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a59cba21",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-43743e1312b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transforms as T\n",
    "import cv2\n",
    "import random\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import ipywidgets as wd\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image, ImageColor\n",
    "import io\n",
    "import torchvision.transforms as transforms\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "DO_TRAINING = False\n",
    "FINAL_MODEL_PATH = \"resources/model_segm_cholecseg8k.pth\"\n",
    "MODEL_NAME = \"fcn_resnet50\" \n",
    "\n",
    "# Defining a color used to depict each semantic class being segmented\n",
    "\n",
    "META_DATA = [\n",
    "    (\"black_background\", (0,0,0)),\n",
    "    (\"abdominal_wall\", (33, 191, 197)),\n",
    "    (\"liver\", (231, 126, 9)),\n",
    "    (\"gastrointestinal_tract\", (209, 53, 84)),\n",
    "    (\"fat\", (80, 155, 4)),\n",
    "    (\"grasper\", (255, 207, 210)),\n",
    "    (\"connective_tissue\", (169, 52, 199)),\n",
    "    (\"blood\", (229, 18, 18)),\n",
    "    (\"cystic_duct\", (149, 50, 18)),\n",
    "    (\"l-hook_electrocautery\", (46, 43, 180)),\n",
    "    (\"gallbladder\", (148, 55, 66)),\n",
    "    (\"hepatic_vein\", (214, 51, 149)),\n",
    "    (\"liver_ligament\", (240, 79, 10)),\n",
    "]\n",
    "\n",
    "COLORS = np.array([m[1] for m in META_DATA]).astype(\"uint8\")\n",
    "\n",
    "# Deining a Look up table used to map each class id representing a semantic class\n",
    "# to it's respective color \n",
    "def get_lut():\n",
    "    lut = np.zeros((256, 1, 3), dtype=np.uint8)\n",
    "    for ii, p in enumerate(COLORS):\n",
    "        lut[ii, 0] = np.array([p[2], p[1], p[0]]).astype(np.uint8)\n",
    "    return lut\n",
    "LUT = get_lut()\n",
    "\n",
    "# Defining some basic transformations\n",
    "IM2TENSOR = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Optimizer parameters\n",
    "learning_rate = 0.00125\n",
    "momentum = 0.9\n",
    "power = 0.9\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab8ca5",
   "metadata": {},
   "source": [
    "## Helper functions and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c5420",
   "metadata": {},
   "source": [
    "Defining some reusable function that we will use throughout this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32d81b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that converts an 2D matrix of class ids to a colored mask representing different\n",
    "# semantic classes\n",
    "def applyCustomColorMap(segmentation_mask):\n",
    "    if len(segmentation_mask.shape) == 2:\n",
    "        segmentation_mask = cv2.cvtColor(segmentation_mask, cv2.COLOR_GRAY2BGR)\n",
    "    im_color = cv2.LUT(segmentation_mask, LUT)\n",
    "    return im_color\n",
    "\n",
    "\n",
    "# Helper function to convert a PIL image to byte array, two formats used by various libraries\n",
    "# to represent images\n",
    "def image_to_byte_array(image):\n",
    "  imgByteArr = io.BytesIO()\n",
    "  image.save(imgByteArr, format=image.format)\n",
    "  imgByteArr = imgByteArr.getvalue()\n",
    "  return imgByteArr\n",
    "\n",
    "\n",
    "def cat_list(images, fill_value=0):\n",
    "    max_size = tuple(max(s) for s in zip(*[img.shape for img in images]))\n",
    "    batch_shape = (len(images),) + max_size\n",
    "    batched_imgs = images[0].new(*batch_shape).fill_(fill_value)\n",
    "    for img, pad_img in zip(images, batched_imgs):\n",
    "        pad_img[..., : img.shape[-2], : img.shape[-1]].copy_(img)\n",
    "    return batched_imgs\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = list(zip(*batch))\n",
    "    batched_imgs = cat_list(images, fill_value=0)\n",
    "    batched_targets = cat_list(targets, fill_value=255)\n",
    "    return batched_imgs, batched_targets\n",
    "\n",
    "# Helper function to do a cross entropy loss between the ground truth and predicted values\n",
    "def criterion(inputs, target):\n",
    "    losses = {}\n",
    "    for name, x in inputs.items():\n",
    "        losses[name] = nn.functional.cross_entropy(x, target, ignore_index=255)\n",
    "    if len(losses) == 1:\n",
    "        return losses[\"out\"]\n",
    "    return losses[\"out\"] + 0.5 * losses[\"aux\"]\n",
    "\n",
    "\n",
    "# Helper function to compute relevant metrics using a confusion matrix\n",
    "# see: https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "class ConfusionMatrix:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.mat = None\n",
    "\n",
    "    def update(self, a, b):\n",
    "        n = self.num_classes\n",
    "        if self.mat is None:\n",
    "            self.mat = torch.zeros((n, n), dtype=torch.int64, device=a.device)\n",
    "        with torch.no_grad():\n",
    "            k = (a >= 0) & (a < n)\n",
    "            inds = n * a[k].to(torch.int64) + b[k]\n",
    "            self.mat += torch.bincount(inds, minlength=n ** 2).reshape(n, n)\n",
    "\n",
    "    def reset(self):\n",
    "        self.mat.zero_()\n",
    "\n",
    "    def compute(self):\n",
    "        h = self.mat.float()\n",
    "        acc_global = torch.diag(h).sum() / h.sum()\n",
    "        acc = torch.diag(h) / h.sum(1)\n",
    "        iou = torch.diag(h) / (h.sum(1) + h.sum(0) - torch.diag(h))\n",
    "        return acc_global, acc, iou\n",
    "    \n",
    "    # Return overall accuracy, per-class accuracy, per-class Intersection over Union (IoU) and mean IoU\n",
    "    def __str__(self):\n",
    "        acc_global, acc, iou = self.compute()\n",
    "        return (\"global correct: {:.1f}\\naverage row correct: {}\\nIoU: {}\\nmean IoU: {:.1f}\").format(\n",
    "            acc_global.item() * 100,\n",
    "            [f\"{i:.1f}\" for i in (acc * 100).tolist()],\n",
    "            [f\"{i:.1f}\" for i in (iou * 100).tolist()],\n",
    "            iou.mean().item() * 100,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c96b6",
   "metadata": {},
   "source": [
    "## CholeSeg8k\n",
    "The CholecSeg8k dataset [1] consists of subset of Cholec80 [2] annotated with semantic segmentation labels with 13 semantic classes for 17 video clips.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1kKJrO75QDINP18gQz8m2CzZ-cDhtuCFk\"/></center>\n",
    "\n",
    "\n",
    "1. _Hong, W-Y., C-L. Kao, Y-H. Kuo, J-R. Wang, W-L. Chang, and C-S. Shih. \"CholecSeg8k: A Semantic Segmentation Dataset for Laparoscopic Cholecystectomy Based on Cholec80.\" arXiv preprint arXiv:2012.12453 (2020)._\n",
    "\n",
    "2. _Twinanda, Andru P., Sherif Shehata, Didier Mutter, Jacques Marescaux, Michel De Mathelin, and Nicolas Padoy. \"Endonet: a deep architecture for recognition tasks on laparoscopic videos.\" IEEE transactions on medical imaging 36, no. 1 (2016): 86-97._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af3530c",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e8beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a dataset class that delivers images and correponding ground truth segmentation masks\n",
    "# from the CholecSeg8k. Please refer to Lecture 6 for more info on torch Datasets.\n",
    "\n",
    "class CholecDatasetSegm(torch.utils.data.Dataset):\n",
    "    def __init__(self, gt_json, meta_data, root_dir = \"./resources/cholecseg8k\", data_split = \"train\", transforms = None):\n",
    "        self.gt_json = gt_json\n",
    "        self.root_dir = root_dir\n",
    "        self.data_split = data_split\n",
    "        self.transforms = transforms\n",
    "        gt_data = json.load(open(gt_json))\n",
    "        self.images = [os.path.join(self.root_dir, g[\"file_name\"]) for g in gt_data]\n",
    "        self.targets = [os.path.join(self.root_dir, g[\"mask_name\"]) for g in gt_data]\n",
    "        self.metadata = meta_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        img = Image.open(self.images[index]).convert(\"RGB\")\n",
    "        target = Image.open(self.targets[index]).convert(\"L\")\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationPresetTrain:\n",
    "    def __init__(self, base_size, crop_size, hflip_prob=0.5, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        min_size = int(0.5 * base_size)\n",
    "        max_size = int(2.0 * base_size)\n",
    "\n",
    "        trans = [T.RandomResize(min_size, max_size)]\n",
    "        if hflip_prob > 0:\n",
    "            trans.append(T.RandomHorizontalFlip(hflip_prob))\n",
    "        trans.extend(\n",
    "            [\n",
    "                T.RandomCrop(crop_size),\n",
    "                T.PILToTensor(),\n",
    "                T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        )\n",
    "        self.transforms = T.Compose(trans)\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        return self.transforms(img, target)\n",
    "\n",
    "\n",
    "class SegmentationPresetEval:\n",
    "    def __init__(self, base_size, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        self.transforms = T.Compose(\n",
    "            [\n",
    "                T.RandomResize(base_size, base_size),\n",
    "                T.PILToTensor(),\n",
    "                T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean=mean, std=std),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        return self.transforms(img, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd7802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Data Loaders for the training and testing splits.\n",
    "# Please refer to Lecture 6 for more info on torch Data Loaders.\n",
    "\n",
    "\n",
    "def get_transform(train=True):\n",
    "    if train:\n",
    "        return SegmentationPresetTrain(base_size=512, crop_size=400)\n",
    "    else:\n",
    "        return SegmentationPresetEval(base_size=400)\n",
    "    \n",
    "# Train loader\n",
    "dataset = CholecDatasetSegm(\"./resources/cholecseg8k/ch80_13vids_train.json\", META_DATA, data_split=\"train\", transforms=get_transform())\n",
    "num_classes = len(META_DATA)\n",
    "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# Test loader\n",
    "dataset_test = CholecDatasetSegm(\"./resources/cholecseg8k/ch80_4vids_val.json\", META_DATA, data_split=\"val\", transforms=get_transform(False))\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, sampler=test_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fc52d",
   "metadata": {},
   "source": [
    "## Segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234be465",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.segmentation.__dict__[MODEL_NAME](pretrained=True)\n",
    "model.classifier[4] = nn.Conv2d(512, num_classes, 1)\n",
    "model.aux_classifier [4] = nn.Conv2d(256, num_classes, 1)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88470175",
   "metadata": {},
   "source": [
    "## Optimizer and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_optimize = [\n",
    "    {\"params\": [p for p in model.backbone.parameters() if p.requires_grad]},\n",
    "    {\"params\": [p for p in model.classifier.parameters() if p.requires_grad]},\n",
    "]\n",
    "params = [p for p in model.aux_classifier.parameters() if p.requires_grad]\n",
    "params_to_optimize.append({\"params\": params, \"lr\": learning_rate * 10})\n",
    "\n",
    "iters_per_epoch = len(data_loader)\n",
    "optimizer = torch.optim.SGD(params_to_optimize, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda x: (1 - x / (iters_per_epoch * NUM_EPOCHS)) ** power)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c47d6",
   "metadata": {},
   "source": [
    "## Helper function for training and validation for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e82f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to train\n",
    "def train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler, device):\n",
    "    model.train()\n",
    "    train_loss  = 0.0\n",
    "    pbar = tqdm(data_loader)\n",
    "    for image, target in pbar:\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_description(\"train_loss: {:.3f} lr: {:.3f}\".format(loss.item(), \n",
    "                                                                    optimizer.param_groups[0][\"lr\"]))\n",
    "    train_loss /= len(data_loader)\n",
    "    return train_loss, optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "# Helper function to evaluate\n",
    "def evaluate(model, data_loader, device, num_classes):\n",
    "    model.eval()\n",
    "    confmat = ConfusionMatrix(num_classes)\n",
    "    pbar = tqdm(data_loader)\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            image, target = image.to(device), target.to(device)\n",
    "            output = model(image)\n",
    "            output = output[\"out\"]\n",
    "            confmat.update(target.flatten(), output.argmax(1).flatten())\n",
    "            pbar.set_description(\"eval\")\n",
    "    return confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d306f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_TRAINING:\n",
    "    pbar = tqdm(range(NUM_EPOCHS))\n",
    "    # Train and evaluate after each epoch\n",
    "    for epoch in pbar:\n",
    "        train_loss, last_lr = train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler, DEVICE)\n",
    "        confmat = evaluate(model, data_loader_test, device=DEVICE, num_classes=num_classes)\n",
    "        acc_global, acc, iu = confmat.compute()    \n",
    "        pbar.set_description(\n",
    "            \"train_loss: {:.3f} last_lr: {:.3f} acc_global: {:.3f} iou: {:.3f}\".format(\n",
    "                train_loss, last_lr, acc_global.item() * 100, iu.mean().item() * 100\n",
    "            )\n",
    "        )\n",
    "        print(\"confmat:\", confmat)\n",
    "        torch.save(model.state_dict(), \"model_epoch_\"+str(epoch)+\".pth\")\n",
    "else:\n",
    "    m,v = model.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=DEVICE))\n",
    "    print(\"=> loaded model weights from {} \\nmissing keys = {}  invalid keys {}\".format(FINAL_MODEL_PATH, m, v))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the video\n",
    "ROOT_DIR = \"resources/cholec80_val_3vids\"\n",
    "VIDEO_PATH_INFERENCE = os.path.join(ROOT_DIR, \"video41\") # or video41 or video42\n",
    "# read the paths of the video frames and sort them to make it sequential\n",
    "video_frames = sorted(\n",
    "    [\n",
    "        int(os.path.basename(a).replace(\".jpg\", \"\"))\n",
    "        for a in glob.glob(VIDEO_PATH_INFERENCE + \"/*.jpg\")\n",
    "    ]\n",
    ")\n",
    "video_frames = [os.path.join(VIDEO_PATH_INFERENCE, str(i) + \".jpg\") for i in video_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = open(video_frames[0], \"rb\").read()\n",
    "lut_image = open(\"resources/lut.jpg\", \"rb\").read()\n",
    "# slider to scroll through the video\n",
    "slider = wd.IntSlider(value=0, min=0, max=len(video_frames) - 1)\n",
    "# play button to plat the video\n",
    "play_button = wd.Play(\n",
    "    value=0, min=0, max=len(video_frames) - 1, step=1, interval=1000\n",
    ")\n",
    "# input_label = wd.Text(value=\"input image\", disabled=True,)\n",
    "# text box to show the model prediction\n",
    "# pred_label = wd.Text(value=\"output prediction\", disabled=True)\n",
    "# image widget to show the image\n",
    "image_wd = wd.Image(value=test_image, width=600, height=336)\n",
    "# image widget to show the output\n",
    "image_wd_out = wd.Image(value=test_image, width=600, height=336)\n",
    "# image widget to show the look up table\n",
    "image_wd_lut = wd.Image(value=lut_image, width=140, height=336)\n",
    "# link the output of the play button to the slider\n",
    "wd.jslink((play_button, \"value\"), (slider, \"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f250c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model in the inference mode\n",
    "model.eval()\n",
    "def slider_update(change):\n",
    "    file_name = video_frames[change.new]\n",
    "    inp_img = Image.open(file_name)\n",
    "    inp_gt = cv2.imread\n",
    "    inp_img_cv2 = cv2.cvtColor(np.array(inp_img), cv2.COLOR_BGR2RGB)\n",
    "    image_wd.value = image_to_byte_array(inp_img)\n",
    "    with torch.no_grad():\n",
    "        image = IM2TENSOR(inp_img)[None].to(DEVICE)\n",
    "        output = model(image)[\"out\"][0].argmax(0).byte().cpu().numpy().astype(\"uint8\")\n",
    "        output_color = applyCustomColorMap(output)\n",
    "        im_output = cv2.addWeighted(inp_img_cv2, 0.5, output_color, 0.5, 0)\n",
    "        image_wd_out.value = cv2.imencode('.png', im_output)[1].tobytes() #image_to_byte_array(Image.fromarray(im_output))\n",
    "#         image_wd_out.value = image_to_byte_array(Image.fromarray(im_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df19051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the app\n",
    "slider.observe(slider_update, \"value\")\n",
    "out = wd.Output()\n",
    "app = wd.VBox(\n",
    "            [\n",
    "                wd.HBox([image_wd, image_wd_out, image_wd_lut]),\n",
    "                wd.HBox([play_button, slider]),\n",
    "            ]\n",
    "        )    \n",
    "display(app)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
